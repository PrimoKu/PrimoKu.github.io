---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a Robotics Master's student at Johns Hopkins University. I am passionate about about leveraging emerging technologies like digital twins, robotics, and mixed reality to address critical challenges in various domains. My research focuses on developing intelligent, computer-assisted systems to enhance precision, efficiency, and decision-making in real-world applications. I am a member of the Advanced Robotics and Computationally Augmented Environments (<a href="https://arcade.cs.jhu.edu" target="_blank" rel="noopener noreferrer">ARCADE</a>) research group, led by Prof. <a href="https://mathiasunberath.github.io" target="_blank" rel="noopener noreferrer">Mathias Unberath</a>.

Currently, I am working on a Mixed Reality platform integrated with NVIDIA Isaac Sim for scalable robotic data generation, and applying imitation learning algorithms for medical robotics to enable autonomous surgical assistance.

In addition, I am collaborating with Prof. <a href="https://www.nursing.psu.edu/directory/pandian/" target="_blank" rel="noopener noreferrer">Vinciya Pandian</a> to develop a high-fidelity virtual reality (VR) system for clinical training, aimed at standardizing procedural workflows and improving training outcomes in healthcare.



<section>
  <h1>Publications</h1>
  (* indicates equal contribution)
  <ul>
    <div style="position: relative;">
      <div id="publication-dart-vinci" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong><em>dARt Vinci: </em>Egocentric Data Collection for Surgical Robot Learning at Scale</strong><br>
        Yihao Liu*, <strong>Yu-Chun Ku*</strong>, Jiaming Zhang*, Hao Ding, Peter Kazanzides, Mehran Armand.<br>
        <em>International Conference on Intelligent Robots and Systems (IROS), 2025.</em> In Review.<br>
        <!-- <a href="https://arxiv.org/abs/2411.13449" target="_blank">Paper</a><br> -->
        <!-- <video width="725" height="450" style="padding: 0 0 15px 0;" controls autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/PrimoKu/PrimoKu.github.io/main/videos/dvrk_comm_loss_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video> -->
        <!-- <p>
          In this paper, we introduce a digital twin system designed to mitigate disruptions caused by communication outages in telesurgery. The system integrates with the da Vinci Research Kit and leverages a dynamic simulation environment to replicate the surgical robot and environment. During communication outages, the surgeon continues operating on the digital twin, with buffered commands replayed to the real robot upon communication restoration. This approach reduces task completion time by 23% and provides a seamless user experience, enabling precise and efficient remote surgical operations even in challenging communication conditions.
        </p> -->
      </li>
    </div>
  </ul>
  <ul>
    <div style="position: relative;">
      <div id="publication-dvrk-commloss" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>A Digital Twin for Telesurgery under Intermittent Communication</strong><br>
        Junxiang Wang, Juan Antonio Barragan, Hisashi Ishida, Jingkai Guo, <strong>Yu-Chun Ku</strong>, Peter Kazanzides.<br>
        <em>International Symposium on Medical Robotics (ISMR), 2025.</em><br>
        <a href="https://arxiv.org/abs/2411.13449" target="_blank">Paper</a><br>
        <video width="725" height="450" style="padding: 0 0 15px 0;" controls autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/PrimoKu/PrimoKu.github.io/main/videos/dvrk_comm_loss_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <!-- <p>
          In this paper, we introduce a digital twin system designed to mitigate disruptions caused by communication outages in telesurgery. The system integrates with the da Vinci Research Kit and leverages a dynamic simulation environment to replicate the surgical robot and environment. During communication outages, the surgeon continues operating on the digital twin, with buffered commands replayed to the real robot upon communication restoration. This approach reduces task completion time by 23% and provides a seamless user experience, enabling precise and efficient remote surgical operations even in challenging communication conditions.
        </p> -->
      </li>
    </div>
  </ul>
  <ul>
    <div style="position: relative;">
      <div id="publication-straighttrack" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>StraightTrack: Towards Mixed Reality Navigation System for Percutaneous K-wire Insertion</strong><br>
        Han Zhang, Benjamin D. Kileen, <strong>Yu-Chun Ku</strong>, Lalithkumar Seenivasan, Yuxuan Zhao, Mingxu Liu, Yue Yang, Suxi Gu, Alejandro Martin-Gomez, Russell H. Taylor, Greg Osgood, Mathias Unberath.<br>
        <em>Wiley Health Technology Letters, 2024. Special Issue: MICCAI AE-CAI 2024</em><br>
        <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/htl2.12103" target="_blank">Paper</a><br>
        <video width="725" height="450" style="padding: 0 0 15px 0;" controls autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/PrimoKu/PrimoKu.github.io/main/videos/straighttrack_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <!-- <p>
          In this paper, we introduce StraightTrack, a mixed reality (MR) system developed to assist in orthopedic guiding wire placement and prevent K-wire bending in complex anatomical regions. The system incorporates a rigid access cannula with an integrated marker body to minimize bending caused by contact with soft tissue or bone. Leveraging an OST HMD with on-device optical tracking, StraightTrack provides real-time surgical guidance while addressing spatial navigation challenges and reducing entry point perception errors.
        </p> -->
      </li>
    </div>
  </ul>
  <ul>
    <div style="position: relative;">
      <div id="publication-outofview-localization" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>Evaluating the Effectiveness of Visual Guidance for Out-of-View Object Localization using Mixed Reality Head-Mounted Displays</strong><br>
        <strong>Yu-Chun Ku</strong>, Alejandro Martin-Gomez.<br>
        <em>International Symposium on Mixed and Augmented Reality (ISMAR), 2024</em><br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10765213" target="_blank">Paper</a><br>
        <img src="/images/evaluate-JND.png" alt="Evaluating JND Image" style="width: 100%; max-width: 725px;">
        <!-- <p>
          In this work, we explore how different visualization techniques for guiding attention toward objects outside a user's field of view impact responsiveness and cognitive workload in MR environments. We examine three visualization paradigms through a user study involving 24 participants. By analyzing Just Noticeable Differences (JNDs) and assessing mental load with the NASA Task Load Index, we aim to determine which technique best balances user perception and cognitive effort.
        </p> -->
      </li>
    </div>
  </ul>
</section>
