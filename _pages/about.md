---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a Robotics Master's student at Johns Hopkins University. I am passionate about leveraging emerging technologies like mixed reality and digital twins to advance healthcare, particularly in interventional procedures. My work focuses on integrating computer-assisted systems to enhance surgical precision and efficiency. Currently, my research includes developing high-fidelity virtual reality system for clinical training and utilizing room-scale digital twin simulation to develop machine learning algorithms.

I am a member of the Advanced Robotics and Computationally Augmented Environments ([ARCADE](https://arcade.cs.jhu.edu)) research group, led by [Mathias Unberath](https://mathiasunberath.github.io). In 2020, I earned a B.A. in Mechatronics Engineering from the National Taiwan Normal University.

<section>
  <h1>Publications</h1>
  <ul>
    <div style="position: relative;">
      <div id="publication-dvrk-commloss" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>A Digital Twin for Telesurgery under Intermittent Communication</strong><br>
        Junxiang Wang, Juan Antonio Barragan, Hisashi Ishida, Jingkai Guo, <strong>Yu-Chun Ku</strong>, Peter Kazanzides.<br>
        <em>The 2025 International Symposium on Medical Robotics.</em> In Review <br>
        <a href="https://arxiv.org/abs/2411.13449" target="_blank">Paper</a><br>
        <video width="725" height="450" style="padding: 0 0 15px 0;" controls autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/PrimoKu/PrimoKu.github.io/main/videos/dvrk_comm_loss_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <!-- <p>
          In this paper, we introduce a digital twin system designed to mitigate disruptions caused by communication outages in telesurgery. The system integrates with the da Vinci Research Kit and leverages a dynamic simulation environment to replicate the surgical robot and environment. During communication outages, the surgeon continues operating on the digital twin, with buffered commands replayed to the real robot upon communication restoration. This approach reduces task completion time by 23% and provides a seamless user experience, enabling precise and efficient remote surgical operations even in challenging communication conditions.
        </p> -->
      </li>
    </div>
  </ul>
  <ul>
    <div style="position: relative;">
      <div id="publication-straighttrack" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>StraightTrack: Towards Mixed Reality Navigation System for Percutaneous K-wire Insertion</strong><br>
        Han Zhang, Benjamin D. Kileen, <strong>Yu-Chun Ku</strong>, Mathias Unberath, et al.<br>
        <em>Wiley Health Technology Letters, 2024. Special Issue: MICCAI AE-CAI 2024</em><br>
        <a href="https://arxiv.org/abs/2410.01143" target="_blank">Paper</a><br>
        <video width="725" height="450" style="padding: 0 0 15px 0;" controls autoplay muted loop>
          <source src="https://media.githubusercontent.com/media/PrimoKu/PrimoKu.github.io/main/videos/straighttrack_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <!-- <p>
          In this paper, we introduce StraightTrack, a mixed reality (MR) system developed to assist in orthopedic guiding wire placement and prevent K-wire bending in complex anatomical regions. The system incorporates a rigid access cannula with an integrated marker body to minimize bending caused by contact with soft tissue or bone. Leveraging an OST HMD with on-device optical tracking, StraightTrack provides real-time surgical guidance while addressing spatial navigation challenges and reducing entry point perception errors.
        </p> -->
      </li>
    </div>
  </ul>
  <ul>
    <div style="position: relative;">
      <div id="publication-outofview-localization" style="position: absolute; top: -120px; visibility: hidden;"></div>
      <li>
        <strong>Evaluating the Effectiveness of Visual Guidance for Out-of-View Object Localization using Mixed Reality Head-Mounted Displays</strong><br>
        <strong>Yu-Chun Ku</strong>, Alejandro Martin-Gomez<br>
        <em>The 2024 IEEE International Symposium on Mixed and Augmented Reality</em><br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10765213" target="_blank">Paper</a><br>
        <img src="/images/evaluate-JND.png" alt="Evaluating JND Image" style="width: 100%; max-width: 725px;">
        <!-- <p>
          In this work, we explore how different visualization techniques for guiding attention toward objects outside a user's field of view impact responsiveness and cognitive workload in MR environments. We examine three visualization paradigms through a user study involving 24 participants. By analyzing Just Noticeable Differences (JNDs) and assessing mental load with the NASA Task Load Index, we aim to determine which technique best balances user perception and cognitive effort.
        </p> -->
      </li>
    </div>
  </ul>
</section>
